# Ficheiro: src/services/content_extractor.py

import logging
import requests
import re
from bs4 import BeautifulSoup
from typing import Optional

# Importa a configura√ß√£o para aceder √† chave da ScrapingAnt
from config import Config

logger = logging.getLogger(__name__)

class ContentExtractor:
    """
    Servi√ßo robusto para extrair o conte√∫do principal de uma p√°gina web.
    Utiliza uma estrat√©gia prim√°ria (ScrapingAnt) e um fallback (requisi√ß√£o direta)
    para garantir a m√°xima chance de sucesso.
    """

    def __init__(self):
        """Inicializa as configura√ß√µes e a chave de API."""
        self.scrapingant_key = Config.SCRAPINGANT_API_KEY
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        logger.info("‚úÖ Content Extractor inicializado.")

    def _clean_text(self, text: str) -> str:
        """
        Limpa o texto extra√≠do, removendo espa√ßos excessivos, quebras de linha
        e linhas curtas que geralmente correspondem a elementos de navega√ß√£o.
        """
        if not text:
            return ""
        
        # Remove m√∫ltiplas quebras de linha, deixando no m√°ximo duas
        text = re.sub(r'\n\s*\n', '\n\n', text)
        # Remove espa√ßos e tabula√ß√µes excessivas
        text = re.sub(r'[ \t]+', ' ', text)
        
        lines = (line.strip() for line in text.splitlines())
        # Mant√©m apenas as linhas com mais de algumas palavras, para focar no conte√∫do principal
        meaningful_lines = [line for line in lines if len(line.split()) > 5]
        
        cleaned_text = '\n'.join(meaningful_lines)
        
        # Limita o tamanho final para n√£o sobrecarregar a IA
        return cleaned_text[:15000]

    def _extract_with_scrapingant(self, url: str) -> Optional[str]:
        """
        Estrat√©gia prim√°ria: usa a API da ScrapingAnt para obter o HTML,
        evitando bloqueios e lidando com p√°ginas renderizadas por JavaScript.
        """
        if not self.scrapingant_key:
            logger.warning("‚ö†Ô∏è Chave da ScrapingAnt n√£o configurada. A saltar esta estrat√©gia.")
            return None
        
        try:
            api_url = f"https://api.scrapingant.com/v2/general"
            params = {'url': url, 'browser': 'false'} # 'browser': 'true' se precisar de renderiza√ß√£o JS
            headers = {'x-api-key': self.scrapingant_key}
            
            response = requests.get(api_url, params=params, headers=headers, timeout=45)
            response.raise_for_status() # Lan√ßa um erro para c√≥digos de status ruins (4xx ou 5xx)

            soup = BeautifulSoup(response.content, 'lxml')
            
            # Remove elementos de "ru√≠do" (scripts, estilos, menus, rodap√©s, etc.)
            for element in soup(["script", "style", "nav", "footer", "header", "aside", "form"]):
                element.decompose()
            
            # Tenta encontrar o conte√∫do principal da p√°gina
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|article|post|body'))
            
            text = main_content.get_text(separator='\n', strip=True) if main_content else soup.get_text(separator='\n', strip=True)
            
            logger.info(f"‚úÖ Conte√∫do extra√≠do com sucesso de {url} via ScrapingAnt.")
            return self._clean_text(text)

        except requests.RequestException as e:
            logger.warning(f"‚ö†Ô∏è Falha na extra√ß√£o com ScrapingAnt para {url}: {e}")
            return None

    def _extract_direct(self, url: str) -> Optional[str]:
        """
        Estrat√©gia de fallback: faz uma requisi√ß√£o HTTP direta para obter o HTML.
        Pode falhar em sites com prote√ß√£o contra scraping.
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=20)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'lxml')
            
            for element in soup(["script", "style", "nav", "footer", "header", "aside", "form"]):
                element.decompose()
            
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|article|post|body'))
            text = main_content.get_text(separator='\n', strip=True) if main_content else soup.get_text(separator='\n', strip=True)
            
            logger.info(f"‚úÖ Conte√∫do extra√≠do com sucesso de {url} via requisi√ß√£o direta.")
            return self._clean_text(text)

        except requests.RequestException as e:
            logger.warning(f"‚ö†Ô∏è Falha na extra√ß√£o direta para {url}: {e}")
            return None

    def extract_content(self, url: str) -> Optional[str]:
        """
        Orquestra a extra√ß√£o de conte√∫do, tentando a melhor estrat√©gia primeiro.
        """
        if not url or not url.startswith('http'):
            logger.warning(f"URL inv√°lida fornecida: {url}")
            return None

        logger.info(f"üöÄ Iniciando extra√ß√£o de conte√∫do para: {url}")
        
        # 1. Tenta a estrat√©gia mais robusta primeiro (ScrapingAnt)
        content = self._extract_with_scrapingant(url)
        
        # 2. Se a primeira falhar, tenta a requisi√ß√£o direta como fallback
        if not content:
            logger.info(f"üîÑ ScrapingAnt falhou. A tentar requisi√ß√£o direta como fallback...")
            content = self._extract_direct(url)
        
        if content and len(content) > 100: # Considera sucesso se o conte√∫do for substancial
            logger.info(f"‚úÖ Extra√ß√£o de conte√∫do para {url} conclu√≠da com sucesso.")
            return content
        else:
            logger.error(f"‚ùå Todas as estrat√©gias de extra√ß√£o falharam para {url}.")
            return None

# --- Inst√¢ncia Global ---
# Cria uma √∫nica inst√¢ncia para ser usada em toda a aplica√ß√£o.
content_extractor = ContentExtractor()
