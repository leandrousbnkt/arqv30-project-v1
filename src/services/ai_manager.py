# Ficheiro: src/services/ai_manager.py

import logging
import requests
import google.generativeai as genai
from typing import Optional, Dict, Any

# Importa a configura√ß√£o centralizada para aceder √†s chaves de API
from config import Config

logger = logging.getLogger(__name__)

class AIManager:
    """
    Gerenciador de IAs com sistema de fallback autom√°tico.
    Prioridade: Google Gemini -> Hugging Face.
    """
    def __init__(self):
        """Inicializa os clientes para os provedores de IA dispon√≠veis."""
        self.providers = {
            'gemini': {'available': False, 'client': None},
            'huggingface': {'available': False}
        }
        
        # --- Inicializa√ß√£o do Google Gemini ---
        if Config.GEMINI_API_KEY:
            try:
                genai.configure(api_key=Config.GEMINI_API_KEY)
                # Usamos um modelo mais recente e eficiente
                self.providers['gemini']['client'] = genai.GenerativeModel("gemini-1.5-flash")
                self.providers['gemini']['available'] = True
                logger.info("‚úÖ Provedor de IA Gemini inicializado com sucesso.")
            except Exception as e:
                logger.error(f"‚ùå Falha ao inicializar o Gemini: {e}")
        else:
            logger.warning("‚ö†Ô∏è Chave de API do Gemini n√£o configurada.")

        # --- Verifica√ß√£o do Hugging Face ---
        if Config.HUGGINGFACE_API_KEY:
            self.providers['huggingface']['available'] = True
            logger.info("‚úÖ Provedor de IA Hugging Face configurado com sucesso.")
        else:
            logger.warning("‚ö†Ô∏è Chave de API do Hugging Face n√£o configurada.")

    def _generate_with_gemini(self, prompt: str, max_tokens: int) -> Optional[str]:
        """Gera conte√∫do usando o Google Gemini como provedor principal."""
        try:
            client = self.providers['gemini']['client']
            
            # Configura√ß√µes de gera√ß√£o para an√°lises detalhadas
            generation_config = {
                'temperature': 0.7,
                'top_p': 0.95,
                'top_k': 64,
                'max_output_tokens': max_tokens,
            }
            
            # Configura√ß√µes de seguran√ßa para evitar bloqueios desnecess√°rios
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ]

            response = client.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            if response.text:
                logger.info(f"‚úÖ An√°lise gerada com sucesso pelo Gemini.")
                return response.text
            else:
                # Se a resposta estiver bloqueada, o motivo estar√° em 'prompt_feedback'
                logger.warning(f"‚ö†Ô∏è Gemini retornou uma resposta vazia. Feedback: {response.prompt_feedback}")
                return None

        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar an√°lise com o Gemini: {e}")
            return None

    def _generate_with_huggingface(self, prompt: str, max_tokens: int) -> Optional[str]:
        """Gera conte√∫do usando a Hugging Face Inference API como fallback."""
        try:
            # Usa o modelo definido na configura√ß√£o, com um fallback padr√£o
            model_name = Config.HUGGINGFACE_MODEL_NAME or "mistralai/Mistral-7B-Instruct-v0.2"
            api_url = f"https://api-inference.huggingface.co/models/{model_name}"
            headers = {"Authorization": f"Bearer {Config.HUGGINGFACE_API_KEY}"}

            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": max_tokens,
                    "temperature": 0.7,
                    "return_full_text": False,
                }
            }
            
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                content = result[0]['generated_text']
                logger.info(f"‚úÖ An√°lise gerada com sucesso pelo Hugging Face ({model_name}).")
                return content
            else:
                logger.error(f"‚ùå Erro na API do Hugging Face: {response.status_code} - {response.text}")
                return None

        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar an√°lise com o Hugging Face: {e}")
            return None

    def generate_analysis(self, prompt: str, max_tokens: int = 8192) -> Optional[str]:
        """
        Orquestra a gera√ß√£o da an√°lise, tentando o provedor prim√°rio (Gemini)
        e recorrendo ao fallback (Hugging Face) se necess√°rio.
        """
        logger.info("üöÄ Iniciando gera√ß√£o de an√°lise com o AI Manager...")
        
        # 1. Tenta o provedor prim√°rio: Gemini
        if self.providers['gemini']['available']:
            logger.info("üß† A tentar provedor prim√°rio: Gemini...")
            result = self._generate_with_gemini(prompt, max_tokens)
            if result:
                return result
            logger.warning("‚ö†Ô∏è Gemini falhou ou retornou resposta vazia. A tentar fallback...")

        # 2. Se o prim√°rio falhar, tenta o fallback: Hugging Face
        if self.providers['huggingface']['available']:
            logger.info("üß† A tentar provedor de fallback: Hugging Face...")
            result = self._generate_with_huggingface(prompt, max_tokens)
            if result:
                return result
            logger.error("‚ùå Fallback com Hugging Face tamb√©m falhou.")

        # 3. Se todos falharem
        logger.critical("‚ùå Todos os provedores de IA falharam. N√£o foi poss√≠vel gerar a an√°lise.")
        return None

# --- Inst√¢ncia Global ---
# Cria uma √∫nica inst√¢ncia do AIManager para ser usada em toda a aplica√ß√£o.
ai_manager = AIManager()
